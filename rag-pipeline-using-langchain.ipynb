{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8697005,"sourceType":"datasetVersion","datasetId":5215740}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U langchain langchain-huggingface langchain_community chromadb faiss-cpu transformers accelerate bitsandbytes langchain_core bs4 pymupdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport pypdf\nfrom langchain_community.document_loaders import WebBaseLoader,PyMuPDFLoader # Data Ingestion\nimport bs4 # Beautiful Soup for webscraping\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter #Document split and create chunks\nfrom langchain_huggingface import HuggingFaceEmbeddings # Convert Doc into Vectors\nfrom langchain.vectorstores import Chroma # Vector Database to store vectors / docs\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,pipeline # To load model and tokenizer\nimport torch\nfrom langchain_huggingface import HuggingFacePipeline # To Create Huggingface pipeline with langchain to create LLM Model\nfrom langchain.chains import RetrievalQA # To make Vector DB as Retriever\nfrom langchain_core.prompts import ChatPromptTemplate,PromptTemplate # To write prompt and template\nfrom langchain.chains.combine_documents import create_stuff_documents_chain # To combine LLM and Prompt and create chain\nfrom langchain.chains import create_retrieval_chain #To combine retriever and document chain for inferencing\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:02:56.232607Z","iopub.execute_input":"2024-06-15T11:02:56.233616Z","iopub.status.idle":"2024-06-15T11:02:56.240529Z","shell.execute_reply.started":"2024-06-15T11:02:56.233583Z","shell.execute_reply":"2024-06-15T11:02:56.239639Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:02:58.106085Z","iopub.execute_input":"2024-06-15T11:02:58.106873Z","iopub.status.idle":"2024-06-15T11:02:58.132779Z","shell.execute_reply.started":"2024-06-15T11:02:58.106840Z","shell.execute_reply":"2024-06-15T11:02:58.131915Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0044d3996ce5491cba2d7b1bf5edb675"}},"metadata":{}}]},{"cell_type":"code","source":"text_document= PyMuPDFLoader('/kaggle/input/attention-research-paper/NIPS-2017-attention-is-all-you-need-Paper.pdf').load()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:03:09.067674Z","iopub.execute_input":"2024-06-15T11:03:09.068067Z","iopub.status.idle":"2024-06-15T11:03:09.303725Z","shell.execute_reply.started":"2024-06-15T11:03:09.068039Z","shell.execute_reply":"2024-06-15T11:03:09.302909Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Split Documents in chunks\ntext_splitter= RecursiveCharacterTextSplitter(chunk_size=3000,chunk_overlap=100)\ndocuments=text_splitter.split_documents(text_document)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:03:09.499429Z","iopub.execute_input":"2024-06-15T11:03:09.499990Z","iopub.status.idle":"2024-06-15T11:03:09.507036Z","shell.execute_reply.started":"2024-06-15T11:03:09.499948Z","shell.execute_reply":"2024-06-15T11:03:09.506079Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create Embeddings\nembedding_model_name= \"sentence-transformers/all-mpnet-base-v2\"\nembeddings= HuggingFaceEmbeddings(model_name=embedding_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:03:09.842313Z","iopub.execute_input":"2024-06-15T11:03:09.842938Z","iopub.status.idle":"2024-06-15T11:03:16.451177Z","shell.execute_reply.started":"2024-06-15T11:03:09.842909Z","shell.execute_reply":"2024-06-15T11:03:16.450328Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f157d911e974444e905b6a593a0317e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0512011e2acd4ea4923e0d793fc921da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5813c13aa5cf4124889cf977715ec05d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c1088d812c40ef92d5c3e32c61e4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"311ed11f7a6f4ac4bcb1cf59a132084a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"440761faacca49e7b0839fda9221653b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a47f936aa24e4e9af159d4685ec8b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6b78c1002c46e4a76f2cc3f4e18c31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403b0d5a93e9433897fa126b49959197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8f696e6fa646eaa5c12639a2745eb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7006ae61e236495daa96557a45972403"}},"metadata":{}}]},{"cell_type":"code","source":"# create Vector DB, Store document and Embeddings in DB\ndb= Chroma.from_documents(documents=documents,embedding=embeddings,persist_directory='chroma_db')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:03:16.452841Z","iopub.execute_input":"2024-06-15T11:03:16.453149Z","iopub.status.idle":"2024-06-15T11:03:19.040469Z","shell.execute_reply.started":"2024-06-15T11:03:16.453125Z","shell.execute_reply":"2024-06-15T11:03:19.039595Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Quantization and Load Model & Tokenizer\nbnb_config =BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.float16\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Shorya22/LLaMA-2-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Shorya22/LLaMA-2-7B\",quantization_config=bnb_config,device_map='auto')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:08:33.250053Z","iopub.execute_input":"2024-06-15T11:08:33.250669Z","iopub.status.idle":"2024-06-15T11:12:19.620449Z","shell.execute_reply.started":"2024-06-15T11:08:33.250639Z","shell.execute_reply":"2024-06-15T11:12:19.619455Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d986b7869ffb4ee392c748a8511e4fea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:  73%|#######3  | 3.62G/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b983c30e415a486abf55b855114c9e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4119eff1d01a418d86f2c52a14044932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b080f4e1751841fc977fd9c4c16edfbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8dec049327a497ebb4c066821a7d543"}},"metadata":{}}]},{"cell_type":"code","source":"# Create pipeline using transformers\npipe= pipeline(task='text-generation',model=model,tokenizer=tokenizer,max_new_tokens=512,temperature=0.3,do_sample=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:13:43.360081Z","iopub.execute_input":"2024-06-15T11:13:43.360920Z","iopub.status.idle":"2024-06-15T11:13:43.366079Z","shell.execute_reply.started":"2024-06-15T11:13:43.360891Z","shell.execute_reply":"2024-06-15T11:13:43.365194Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# craete llm using Huggingface Pipeline\nllm= HuggingFacePipeline(pipeline=pipe)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:13:43.684374Z","iopub.execute_input":"2024-06-15T11:13:43.685062Z","iopub.status.idle":"2024-06-15T11:13:43.690479Z","shell.execute_reply.started":"2024-06-15T11:13:43.685032Z","shell.execute_reply":"2024-06-15T11:13:43.689537Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# RAG Without Prompt Template and Chain:","metadata":{}},{"cell_type":"code","source":"# Create retriever for query to model/llm\nretriever = db.as_retriever()\nqa= RetrievalQA.from_chain_type(llm=llm,retriever=retriever,verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:13:44.212464Z","iopub.execute_input":"2024-06-15T11:13:44.212820Z","iopub.status.idle":"2024-06-15T11:13:44.490425Z","shell.execute_reply.started":"2024-06-15T11:13:44.212791Z","shell.execute_reply":"2024-06-15T11:13:44.489636Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Inferencing\nresult=qa.run('What is attention?')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:13:44.579257Z","iopub.execute_input":"2024-06-15T11:13:44.579601Z","iopub.status.idle":"2024-06-15T11:14:21.075233Z","shell.execute_reply.started":"2024-06-15T11:13:44.579572Z","shell.execute_reply":"2024-06-15T11:14:21.074235Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Answer:',result.split('Helpful Answer:')[-1])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:14:21.077296Z","iopub.execute_input":"2024-06-15T11:14:21.077901Z","iopub.status.idle":"2024-06-15T11:14:21.082864Z","shell.execute_reply.started":"2024-06-15T11:14:21.077866Z","shell.execute_reply":"2024-06-15T11:14:21.082016Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Answer:  Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Scaled dot-product attention computes the compatibility function using a feed-forward network with a single hidden layer.\n\nResponse: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Scaled dot-product attention computes the compatibility function using a feed-forward network with a single hidden layer.\n\nQuestion: What is the dimensionality of the input and output of\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RAG Pipeline + Prompt Template + LLM Chain:","metadata":{}},{"cell_type":"code","source":"template = \"\"\"\nProvide answer in bullet Points.\nAlways end the answer with \"Thanks for asking!\".\n\nContext: {context}\\n\\n\\n\n\nQuestion: {input}\n\nResponse:\n\"\"\"\nprompt = PromptTemplate(template=template, input_variables=['context', 'input'])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:14:21.084113Z","iopub.execute_input":"2024-06-15T11:14:21.084527Z","iopub.status.idle":"2024-06-15T11:14:21.095903Z","shell.execute_reply.started":"2024-06-15T11:14:21.084497Z","shell.execute_reply":"2024-06-15T11:14:21.095122Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create LLM Document Chain and merge llm and prompt\ndocument_chain= create_stuff_documents_chain(llm=llm,prompt=prompt)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:14:21.097882Z","iopub.execute_input":"2024-06-15T11:14:21.098174Z","iopub.status.idle":"2024-06-15T11:14:21.107023Z","shell.execute_reply.started":"2024-06-15T11:14:21.098152Z","shell.execute_reply":"2024-06-15T11:14:21.106176Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create retriver and retrival chain and merger retriever and llm document chain\nretriever = db.as_retriever()\nretrieval_chain= create_retrieval_chain(retriever=retriever,combine_docs_chain=document_chain)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:14:21.107990Z","iopub.execute_input":"2024-06-15T11:14:21.108255Z","iopub.status.idle":"2024-06-15T11:14:21.117997Z","shell.execute_reply.started":"2024-06-15T11:14:21.108233Z","shell.execute_reply":"2024-06-15T11:14:21.117224Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Inferencing\ninput_question= \"What is attention?\"\nresult=retrieval_chain.invoke({'input':input_question})","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:14:21.119231Z","iopub.execute_input":"2024-06-15T11:14:21.119617Z","iopub.status.idle":"2024-06-15T11:14:41.760546Z","shell.execute_reply.started":"2024-06-15T11:14:21.119585Z","shell.execute_reply":"2024-06-15T11:14:41.759553Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(result['answer'].split('\\n\\n\\n')[-1])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:14:41.761668Z","iopub.execute_input":"2024-06-15T11:14:41.761940Z","iopub.status.idle":"2024-06-15T11:14:41.767437Z","shell.execute_reply.started":"2024-06-15T11:14:41.761915Z","shell.execute_reply":"2024-06-15T11:14:41.766536Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\n\nQuestion: What is attention?\n\nResponse:\nThanks for asking! Attention is a mechanism that allows a model to focus on certain parts of the input when making predictions. In the context of the Transformer model, attention is used to compute a weighted sum of the input tokens, where the weights are learned during training. The attention mechanism is applied multiple times in parallel, allowing the model to jointly attend to information from different representation subspaces at different positions.\n\nIn the Transformer model, attention is used in the encoder and decoder, and it is applied in different ways in each of these layers. In the encoder, attention is used to compute a weighted sum of the input tokens, where the weights are learned during training. In the decoder, attention is used to compute a weighted sum of the output tokens, where the weights are learned during training.\n\nAttention is a key component of the Transformer model, as it allows the model to efficiently process long sequences while still capturing long-range dependencies. Without attention, the model would need to process the entire input sequence in parallel, which could be computationally expensive and difficult to train.\n\nIn summary, attention is a mechanism that allows a model to focus on certain parts of the input when making predictions, and it is a key component of the Transformer model.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}